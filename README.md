# LOR-3000‚Ñ¢

## üß≠ Summary

**LLM Orchestrator 3000‚Ñ¢** is a backend-for-frontend service that unifies access to multiple LLM providers through a compiler-inspired architecture. It is built for frontend teams and external systems to interact with GPT-4, Claude, and future LLMs using a single, simple, and flexible HTTP API ‚Äî without worrying about rate limits, prompt structuring, cost control, or failover logic.

This system allows:

* Provider abstraction (OpenAI, Claude, etc.)
* Smart fallback logic
* Versioned, composable prompt handling (Prompt Oven‚Ñ¢)
* Session/thread/message storage (ThreadCore‚Ñ¢)
* Feedback collection (EchoChamber‚Ñ¢)
* Persistent memory context for chats (MemoryLane‚Ñ¢)
* Observability & telemetry (Sentinel‚Ñ¢)
* Flexible response formatting (OutputSmith‚Ñ¢)

It‚Äôs modular, scalable, and designed to evolve into a full LLM runtime engine with RAG, tool use (FuncForge‚Ñ¢), streaming, and dynamic pipelines.

### üîß Primary Use Cases

* Frontend apps that call LLMs and need consistency, structure, and memory
* Teams building AI assistants or tools across vendors
* Multi-turn chat history and versioned prompt control
* Rate-limiting and cost-controlled API access

### üß± Architecture Overview

```mermaid
graph TD
    A[Frontend App / UI] --> B[HTTP API: /chat]
    B --> C[Router Engine]
    C --> D[Prompt Oven‚Ñ¢]
    C --> E[ThreadCore‚Ñ¢]
    C --> F[OutputSmith‚Ñ¢]
    C --> G[Sentinel‚Ñ¢]
    D --> H[LLM Providers: GPT-4, Claude]
    E --> H
    F --> H
    G --> H
```

---

## üéØ Overview

LLM Orchestrator 3000‚Ñ¢ is a **backend-for-frontend** service that provides a unified, compiler-inspired abstraction layer over multiple LLM providers. It acts as a smart LLM gateway ‚Äî handling routing, fallback, formatting, and token/cost management ‚Äî so that frontend developers or external systems can interact with LLMs using a simple, consistent API.

---

## üìå Goals (v1.0)

* Support **2 initial LLMs**: OpenAI GPT-4 and Claude 3 Opus
* Provide a unified HTTP API for frontend consumption
* Abstract provider SDKs, tokens, and fallback logic behind a clean interface
* Support fallback mechanism between models
* Output formatting: raw, markdown, json (via OutputSmith‚Ñ¢)
* Minimal CLI (dev tool only, optional)
* Static JSON/YAML config
* Versioned prompt manager with Redis cache (Prompt Oven‚Ñ¢)
* PostgreSQL-backed session/thread/message storage (ThreadCore‚Ñ¢)
* Internal observer module for metrics/logging/monitoring (Sentinel‚Ñ¢)
* Feedback collection layer for rating messages (EchoChamber‚Ñ¢) *(post-v1.0)*
* Stateful thread context injection (MemoryLane‚Ñ¢) *(post-v1.0)*

---

## üì¶ Key Features

### ‚úÖ REST API

* `POST /chat` ‚Üí Handle prompt and return LLM response
* `GET /config` ‚Üí Return supported models, settings (for frontend UI)
* `POST /pipeline` *(optional v1.1+)* ‚Üí Run multi-step prompt chains

### ‚úÖ Provider Abstraction (Frontend Layer)

* Normalize interaction with OpenAI and Claude APIs
* Shared `Provider` interface
* Hidden API key, token cost, and error handling logic

### ‚úÖ Routing Engine (Middle-End)

* Chooses provider based on config
* Tries fallback on error/quota
* Logs usage & cost

### ‚úÖ Compiler Passes (Middleware)

* TokenBudgetPass (ensure prompt fits model limits)
* Format enforcement (if needed)

### ‚úÖ OutputSmith‚Ñ¢ (Backend Formatter)

* Raw, JSON, Markdown formatting options
* Pluggable interface for future custom formatters
* Optional stream handling (v1.1+)

### ‚úÖ Dev CLI (Optional)

* `lor3 run --prompt="..."` ‚Üí Local dev use only

### ‚úÖ Prompt Oven‚Ñ¢ (Prompt Manager)

* Load system prompts from JSON/YAML config
* Store compiled prompts in Redis
* Support versioned prompts (e.g. `support_agent:v1`, `rag_summary:v3`)
* Optional syncing with OpenAI Assistants API
* CLI tooling:

  * `lor3 prompt list`
  * `lor3 prompt show <name>`
  * `lor3 prompt edit <name>`
  * `lor3 prompt sync <name>`
  * `lor3 prompt import --from-assistant-id=<id>`
* Prompt composition support (fragments/macros)

### ‚úÖ ThreadCore‚Ñ¢ (Database-backed Session/Message Tracking)

* PostgreSQL database
* Tables:

  * `sessions` ‚Üí group by user
  * `threads` ‚Üí conversation thread context
  * `messages` ‚Üí full chat history
  * `prompt_versions` ‚Üí system prompt snapshots
* Use cases:

  * Retrieve message context for multi-turn chats
  * Track model used, prompt version, token cost per message
  * Enable future analytics, prompt tuning, reverse learning

### ‚úÖ Sentinel‚Ñ¢ (Internal Observer/Logger)

* Tracks and logs all orchestration activity
* Logs:

  * Provider usage
  * Prompt version used
  * Token and cost stats
  * Failovers triggered
* Optional metrics:

  * Prompts per model
  * Error rates
  * Token spikes or cost anomalies
* Exports:

  * Local file logs
  * stdout
  * Prometheus endpoint (optional v1.1+)
* CLI tooling:

  * `lor3 observe usage`
  * `lor3 observe prompts`
  * `lor3 observe providers`
  * `lor3 observe errors`

### ‚úÖ EchoChamber‚Ñ¢ (Feedback Loop Engine ‚Äî Post v1.0)

* Let frontend apps send feedback on assistant replies
* Adds `feedback` and `rating` fields to messages
* API to submit feedback on specific message ID
* Stored in PG alongside session/thread/message
* Enables:

  * Prompt improvement
  * Reverse learning
  * Analytics on assistant usefulness
* Planned CLI support:

  * `lor3 feedback rate <message_id> --rating thumbs_down --comment "Too vague"`
  * `lor3 feedback show <thread_id>`

### ‚úÖ MemoryLane‚Ñ¢ (Stateful Thread Context ‚Äî Post v1.0)

* Allows multi-turn chat memory via thread context
* Pulls last `N` messages from thread and injects into request
* Configurable via `context_depth`
* CLI/HTTP param: `--context-depth=5`
* Use cases:

  * Multi-step chat UIs
  * Follow-up queries
  * Assistant continuity

---

## üß± Architecture (Inspired by GCC)

```
[Frontend Request] --> [Router] --> [Provider Adapter] --> [LLM API]
                             ‚Üì
                 [Compiler Passes / Middleware]
                             ‚Üì
  [Prompt Oven‚Ñ¢] + [ThreadCore‚Ñ¢] + [OutputSmith‚Ñ¢] + [Sentinel‚Ñ¢] + [EchoChamber‚Ñ¢] + [MemoryLane‚Ñ¢]
```

### Modules:

* `/frontend/` ‚Üí Providers: `openai.go`, `claude.go`
* `/middleend/` ‚Üí Router + passes: `router.go`, `tokenbudget.go`
* `/backend/` ‚Üí OutputSmith‚Ñ¢ formatter module
* `/runtime/` ‚Üí Config, keys, usage state
* `/oven/` ‚Üí Prompt manager, Redis cache, CLI assistant sync
* `/threadcore/` ‚Üí PostgreSQL models, queries, session/message handling
* `/sentinel/` ‚Üí Observer module for logging, metrics, telemetry
* `/echochamber/` ‚Üí Feedback engine for user ratings *(v1.1+)*
* `/memorylane/` ‚Üí Thread context memory injection *(v1.1+)*
* `/api/` ‚Üí HTTP endpoints
* `/cli/` ‚Üí Dev tool wrapper (optional)

---

## ‚öôÔ∏è Tech Stack

* Language: **Go**
* HTTP Server: `net/http`, possibly `chi` or `fiber`
* Config: `viper`, env or JSON
* Logging: `zerolog`
* Redis for prompt cache
* PostgreSQL for session and message history (via `pgx` + migrations)
* RESTful API (OpenAPI spec planned)

---

## üîÅ Version Plan

| Version | Feature                                                                                |
| ------- | -------------------------------------------------------------------------------------- |
| v1.0    | GPT-4 + Claude, REST API, fallback, Prompt Oven‚Ñ¢, ThreadCore‚Ñ¢, OutputSmith‚Ñ¢, Sentinel‚Ñ¢ |
| v1.1    | EchoChamber‚Ñ¢, MemoryLane‚Ñ¢, optional streaming support                                  |
| v1.5    | Add Gemini support                                                                     |
| v1.6    | Add Mistral support                                                                    |
| v2.0    | Plugin system for dynamic provider loading                                             |
| v3.0    | Web UI (external project, frontend consumer)                                           |

---

## üßæ Config Example (v1.0)

```json
{
  "primary": "openai:gpt-4",
  "fallbacks": ["claude:opus"],
  "passes": ["token-budget-check"],
  "format": "markdown",
  "max_tokens": 4000
}
```

---

## üìå Non-Goals (v1.0)

* No user authentication (yet)
* No streaming support (planned for 1.1+)
* No UI / frontend in this project
* No plugin hot-reload

---

## üìÖ Milestones

| Milestone     | Description                                 |
| ------------- | ------------------------------------------- |
| v1.0-init     | Project skeleton + config system            |
| v1.0-core     | Providers + router + fallback               |
| v1.0-api      | HTTP endpoints implemented                  |
| v1.0-oven     | Prompt Oven‚Ñ¢ and Redis cache complete       |
| v1.0-thread   | PostgreSQL schema + ThreadCore‚Ñ¢ integration |
| v1.0-sentinel | Observer module logs + CLI metrics complete |
| v1.0-done     | Testing, logging, docs done                 |

---

## ‚úÖ Success Criteria

* `POST /chat` accepts prompt and returns generated response
* Response formats respected: raw, markdown, json
* If GPT-4 fails, fallback to Claude 3 Opus
* Frontend teams don‚Äôt need to touch LLM APIs directly
* Prompts can be versioned, cached, and composed
* Session/thread/message data is saved to PostgreSQL
* Sentinel logs provider usage, fallback, token/cost info
* EchoChamber‚Ñ¢ collects feedback per message and stores in DB
* MemoryLane‚Ñ¢ injects previous message context when enabled
* Easy to add new providers in v1.5+ without refactoring core
